{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0824628b-05e8-4d38-98d0-b6234ac80750",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Core libraries and model APIs\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold, cross_val_score\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import balanced_accuracy_score, f1_score, classification_report\n",
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "from sklearn.utils.class_weight import compute_class_weight, compute_sample_weight\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "757e0d23-0d78-4308-a3e5-4e086283d67f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>b</th>\n",
       "      <th>e</th>\n",
       "      <th>AC</th>\n",
       "      <th>FM</th>\n",
       "      <th>UC</th>\n",
       "      <th>DL</th>\n",
       "      <th>DS</th>\n",
       "      <th>DP</th>\n",
       "      <th>DR</th>\n",
       "      <th>Unnamed: 9</th>\n",
       "      <th>...</th>\n",
       "      <th>E</th>\n",
       "      <th>AD</th>\n",
       "      <th>DE</th>\n",
       "      <th>LD</th>\n",
       "      <th>FS</th>\n",
       "      <th>SUSP</th>\n",
       "      <th>Unnamed: 42</th>\n",
       "      <th>CLASS</th>\n",
       "      <th>Unnamed: 44</th>\n",
       "      <th>NSP</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>240.0</td>\n",
       "      <td>357.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>9.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5.0</td>\n",
       "      <td>632.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>6.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>177.0</td>\n",
       "      <td>779.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>6.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>411.0</td>\n",
       "      <td>1192.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>6.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>533.0</td>\n",
       "      <td>1147.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2124</th>\n",
       "      <td>1576.0</td>\n",
       "      <td>3049.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2125</th>\n",
       "      <td>2796.0</td>\n",
       "      <td>3415.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2126</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2127</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>72.0</td>\n",
       "      <td>332.0</td>\n",
       "      <td>252.0</td>\n",
       "      <td>107.0</td>\n",
       "      <td>69.0</td>\n",
       "      <td>197.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2128</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>564.0</td>\n",
       "      <td>23.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2129 rows × 46 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           b       e   AC     FM    UC    DL   DS   DP   DR  Unnamed: 9  ...  \\\n",
       "0      240.0   357.0  0.0    0.0   0.0   0.0  0.0  0.0  0.0         NaN  ...   \n",
       "1        5.0   632.0  4.0    0.0   4.0   2.0  0.0  0.0  0.0         NaN  ...   \n",
       "2      177.0   779.0  2.0    0.0   5.0   2.0  0.0  0.0  0.0         NaN  ...   \n",
       "3      411.0  1192.0  2.0    0.0   6.0   2.0  0.0  0.0  0.0         NaN  ...   \n",
       "4      533.0  1147.0  4.0    0.0   5.0   0.0  0.0  0.0  0.0         NaN  ...   \n",
       "...      ...     ...  ...    ...   ...   ...  ...  ...  ...         ...  ...   \n",
       "2124  1576.0  3049.0  1.0    0.0   9.0   0.0  0.0  0.0  0.0         NaN  ...   \n",
       "2125  2796.0  3415.0  1.0    1.0   5.0   0.0  0.0  0.0  0.0         NaN  ...   \n",
       "2126     NaN     NaN  NaN    NaN   NaN   NaN  NaN  NaN  NaN         NaN  ...   \n",
       "2127     NaN     NaN  NaN    NaN   NaN   0.0  0.0  0.0  0.0         NaN  ...   \n",
       "2128     NaN     NaN  NaN  564.0  23.0  16.0  1.0  4.0  0.0         NaN  ...   \n",
       "\n",
       "         E     AD     DE     LD    FS   SUSP  Unnamed: 42  CLASS  Unnamed: 44  \\\n",
       "0     -1.0   -1.0   -1.0   -1.0   1.0   -1.0          NaN    9.0          NaN   \n",
       "1     -1.0    1.0   -1.0   -1.0  -1.0   -1.0          NaN    6.0          NaN   \n",
       "2     -1.0    1.0   -1.0   -1.0  -1.0   -1.0          NaN    6.0          NaN   \n",
       "3     -1.0    1.0   -1.0   -1.0  -1.0   -1.0          NaN    6.0          NaN   \n",
       "4     -1.0   -1.0   -1.0   -1.0  -1.0   -1.0          NaN    2.0          NaN   \n",
       "...    ...    ...    ...    ...   ...    ...          ...    ...          ...   \n",
       "2124   1.0   -1.0   -1.0   -1.0  -1.0   -1.0          NaN    5.0          NaN   \n",
       "2125  -1.0   -1.0   -1.0   -1.0  -1.0   -1.0          NaN    1.0          NaN   \n",
       "2126   NaN    NaN    NaN    NaN   NaN    NaN          NaN    NaN          NaN   \n",
       "2127  72.0  332.0  252.0  107.0  69.0  197.0          NaN    NaN          NaN   \n",
       "2128   NaN    NaN    NaN    NaN   NaN    NaN          NaN    NaN          NaN   \n",
       "\n",
       "      NSP  \n",
       "0     2.0  \n",
       "1     1.0  \n",
       "2     1.0  \n",
       "3     1.0  \n",
       "4     1.0  \n",
       "...   ...  \n",
       "2124  2.0  \n",
       "2125  1.0  \n",
       "2126  NaN  \n",
       "2127  NaN  \n",
       "2128  NaN  \n",
       "\n",
       "[2129 rows x 46 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_excel('ctg.xlsx', sheet_name = 'Data', header = 1)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "38c3b314-1a6d-4fab-b348-ec53cac25fd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_data(df: pd.DataFrame):\n",
    "    \"\"\"\n",
    "    Cleans the input dataframe by:\n",
    "    1. Removing empty rows and columns.\n",
    "    2. Keeping only the specified features and the target.\n",
    "    3. Removing duplicate rows.\n",
    "    4. Dropping rows with missing target values.\n",
    "    5. Splitting into features (X) and target (y).\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): The raw dataframe.\n",
    "\n",
    "    Returns:\n",
    "        X (pd.DataFrame): Feature dataframe (restricted to chosen features).\n",
    "        y (pd.Series): Target column ('NSP').\n",
    "    \"\"\"\n",
    "\n",
    "    # Step 1: Make a working copy of the dataframe\n",
    "    cleaned = df.copy()\n",
    "\n",
    "    # Step 2: Drop any rows or columns that are completely empty\n",
    "    cleaned = cleaned.dropna(axis=0, how='all').dropna(axis=1, how='all')\n",
    "\n",
    "    # Step 3: Define target and allowed features\n",
    "    target_col = 'NSP'\n",
    "    allowed_features = {\n",
    "        'b', 'e', 'AC', 'FM', 'UC', 'DL', 'DS', 'DP', 'DR', 'LB',\n",
    "        'ASTV', 'MSTV', 'ALTV', 'MLTV',\n",
    "        'Width', 'Min', 'Max', 'Nmax', 'Nzeros', 'Mode',\n",
    "        'Median', 'Variance', 'Tendency'\n",
    "    }\n",
    "\n",
    "    # Step 4: Keep only allowed features + target\n",
    "    keep_cols = list(allowed_features) + [target_col]\n",
    "    cleaned = cleaned.loc[:, [c for c in cleaned.columns if c in keep_cols]]\n",
    "\n",
    "    # Step 5: Remove duplicate rows\n",
    "    cleaned = cleaned.drop_duplicates()\n",
    "\n",
    "    # Step 6: Drop rows where target is missing\n",
    "    cleaned = cleaned.dropna(subset=[target_col])\n",
    "\n",
    "    # Step 7: Separate features and target\n",
    "    X = cleaned.drop(columns=[target_col])\n",
    "    y = cleaned[target_col]\n",
    "\n",
    "    return X, y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "725391bd-46f0-401b-9338-e18a72834fca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>b</th>\n",
       "      <th>e</th>\n",
       "      <th>AC</th>\n",
       "      <th>FM</th>\n",
       "      <th>UC</th>\n",
       "      <th>DL</th>\n",
       "      <th>DS</th>\n",
       "      <th>DP</th>\n",
       "      <th>DR</th>\n",
       "      <th>LB</th>\n",
       "      <th>...</th>\n",
       "      <th>MLTV</th>\n",
       "      <th>Width</th>\n",
       "      <th>Min</th>\n",
       "      <th>Max</th>\n",
       "      <th>Nmax</th>\n",
       "      <th>Nzeros</th>\n",
       "      <th>Mode</th>\n",
       "      <th>Median</th>\n",
       "      <th>Variance</th>\n",
       "      <th>Tendency</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>240.0</td>\n",
       "      <td>357.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>120.0</td>\n",
       "      <td>...</td>\n",
       "      <td>2.4</td>\n",
       "      <td>64.0</td>\n",
       "      <td>62.0</td>\n",
       "      <td>126.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>120.0</td>\n",
       "      <td>121.0</td>\n",
       "      <td>73.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5.0</td>\n",
       "      <td>632.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>132.0</td>\n",
       "      <td>...</td>\n",
       "      <td>10.4</td>\n",
       "      <td>130.0</td>\n",
       "      <td>68.0</td>\n",
       "      <td>198.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>141.0</td>\n",
       "      <td>140.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>177.0</td>\n",
       "      <td>779.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>133.0</td>\n",
       "      <td>...</td>\n",
       "      <td>13.4</td>\n",
       "      <td>130.0</td>\n",
       "      <td>68.0</td>\n",
       "      <td>198.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>141.0</td>\n",
       "      <td>138.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>411.0</td>\n",
       "      <td>1192.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>134.0</td>\n",
       "      <td>...</td>\n",
       "      <td>23.0</td>\n",
       "      <td>117.0</td>\n",
       "      <td>53.0</td>\n",
       "      <td>170.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>137.0</td>\n",
       "      <td>137.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>533.0</td>\n",
       "      <td>1147.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>132.0</td>\n",
       "      <td>...</td>\n",
       "      <td>19.9</td>\n",
       "      <td>117.0</td>\n",
       "      <td>53.0</td>\n",
       "      <td>170.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>137.0</td>\n",
       "      <td>138.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2121</th>\n",
       "      <td>2059.0</td>\n",
       "      <td>2867.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>140.0</td>\n",
       "      <td>...</td>\n",
       "      <td>7.2</td>\n",
       "      <td>40.0</td>\n",
       "      <td>137.0</td>\n",
       "      <td>177.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>153.0</td>\n",
       "      <td>152.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2122</th>\n",
       "      <td>1576.0</td>\n",
       "      <td>2867.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>140.0</td>\n",
       "      <td>...</td>\n",
       "      <td>7.1</td>\n",
       "      <td>66.0</td>\n",
       "      <td>103.0</td>\n",
       "      <td>169.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>152.0</td>\n",
       "      <td>151.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2123</th>\n",
       "      <td>1576.0</td>\n",
       "      <td>2596.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>140.0</td>\n",
       "      <td>...</td>\n",
       "      <td>6.1</td>\n",
       "      <td>67.0</td>\n",
       "      <td>103.0</td>\n",
       "      <td>170.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>153.0</td>\n",
       "      <td>152.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2124</th>\n",
       "      <td>1576.0</td>\n",
       "      <td>3049.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>140.0</td>\n",
       "      <td>...</td>\n",
       "      <td>7.0</td>\n",
       "      <td>66.0</td>\n",
       "      <td>103.0</td>\n",
       "      <td>169.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>152.0</td>\n",
       "      <td>151.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2125</th>\n",
       "      <td>2796.0</td>\n",
       "      <td>3415.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>142.0</td>\n",
       "      <td>...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>42.0</td>\n",
       "      <td>117.0</td>\n",
       "      <td>159.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>145.0</td>\n",
       "      <td>145.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2115 rows × 23 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           b       e   AC   FM   UC   DL   DS   DP   DR     LB  ...  MLTV  \\\n",
       "0      240.0   357.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  120.0  ...   2.4   \n",
       "1        5.0   632.0  4.0  0.0  4.0  2.0  0.0  0.0  0.0  132.0  ...  10.4   \n",
       "2      177.0   779.0  2.0  0.0  5.0  2.0  0.0  0.0  0.0  133.0  ...  13.4   \n",
       "3      411.0  1192.0  2.0  0.0  6.0  2.0  0.0  0.0  0.0  134.0  ...  23.0   \n",
       "4      533.0  1147.0  4.0  0.0  5.0  0.0  0.0  0.0  0.0  132.0  ...  19.9   \n",
       "...      ...     ...  ...  ...  ...  ...  ...  ...  ...    ...  ...   ...   \n",
       "2121  2059.0  2867.0  0.0  0.0  6.0  0.0  0.0  0.0  0.0  140.0  ...   7.2   \n",
       "2122  1576.0  2867.0  1.0  0.0  9.0  0.0  0.0  0.0  0.0  140.0  ...   7.1   \n",
       "2123  1576.0  2596.0  1.0  0.0  7.0  0.0  0.0  0.0  0.0  140.0  ...   6.1   \n",
       "2124  1576.0  3049.0  1.0  0.0  9.0  0.0  0.0  0.0  0.0  140.0  ...   7.0   \n",
       "2125  2796.0  3415.0  1.0  1.0  5.0  0.0  0.0  0.0  0.0  142.0  ...   5.0   \n",
       "\n",
       "      Width    Min    Max  Nmax  Nzeros   Mode  Median  Variance  Tendency  \n",
       "0      64.0   62.0  126.0   2.0     0.0  120.0   121.0      73.0       1.0  \n",
       "1     130.0   68.0  198.0   6.0     1.0  141.0   140.0      12.0       0.0  \n",
       "2     130.0   68.0  198.0   5.0     1.0  141.0   138.0      13.0       0.0  \n",
       "3     117.0   53.0  170.0  11.0     0.0  137.0   137.0      13.0       1.0  \n",
       "4     117.0   53.0  170.0   9.0     0.0  137.0   138.0      11.0       1.0  \n",
       "...     ...    ...    ...   ...     ...    ...     ...       ...       ...  \n",
       "2121   40.0  137.0  177.0   4.0     0.0  153.0   152.0       2.0       0.0  \n",
       "2122   66.0  103.0  169.0   6.0     0.0  152.0   151.0       3.0       1.0  \n",
       "2123   67.0  103.0  170.0   5.0     0.0  153.0   152.0       4.0       1.0  \n",
       "2124   66.0  103.0  169.0   6.0     0.0  152.0   151.0       4.0       1.0  \n",
       "2125   42.0  117.0  159.0   2.0     1.0  145.0   145.0       1.0       0.0  \n",
       "\n",
       "[2115 rows x 23 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X, y = clean_data(df)\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2e212996-3eb1-4221-97fe-11f69c0a2090",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['b', 'e', 'AC', 'FM', 'UC', 'DL', 'DS', 'DP', 'DR', 'LB', 'ASTV',\n",
       "       'MSTV', 'ALTV', 'MLTV', 'Width', 'Min', 'Max', 'Nmax', 'Nzeros', 'Mode',\n",
       "       'Median', 'Variance', 'Tendency'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a6c5e751-5bd9-41c6-947d-ee79519a906c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       2.0\n",
       "1       1.0\n",
       "2       1.0\n",
       "3       1.0\n",
       "4       1.0\n",
       "       ... \n",
       "2121    2.0\n",
       "2122    2.0\n",
       "2123    2.0\n",
       "2124    2.0\n",
       "2125    1.0\n",
       "Name: NSP, Length: 2115, dtype: float64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a29ba784-e53e-45d9-9ea9-cf252c81c583",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import StratifiedKFold, cross_val_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import make_scorer, balanced_accuracy_score\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.pipeline import Pipeline\n",
    "\n",
    "# --- Example function to run SMOTE + CV ---\n",
    "def evaluate_with_smote_cv(X, y, n_splits=5, random_state=42):\n",
    "    \"\"\"\n",
    "    Evaluates a RandomForest model with SMOTE oversampling applied inside CV folds.\n",
    "    Returns balanced accuracy scores for each fold.\n",
    "    \"\"\"\n",
    "\n",
    "    # Define pipeline: SMOTE applied only to training folds\n",
    "    pipeline = Pipeline(steps=[\n",
    "        ('smote', SMOTE(random_state=random_state)),\n",
    "        ('rf', RandomForestClassifier(\n",
    "            n_estimators=300,\n",
    "            class_weight='balanced',\n",
    "            random_state=random_state,\n",
    "            n_jobs=-1\n",
    "        ))\n",
    "    ])\n",
    "\n",
    "    # Stratified 5-fold split\n",
    "    skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=random_state)\n",
    "\n",
    "    # Use balanced accuracy (good for imbalanced data)\n",
    "    scorer = make_scorer(balanced_accuracy_score)\n",
    "\n",
    "    # Cross-validation with SMOTE inside each training fold\n",
    "    scores = cross_val_score(pipeline, X, y, cv=skf, scoring=scorer, n_jobs=-1)\n",
    "\n",
    "    print(\"Balanced Accuracy per fold:\", np.round(scores, 3))\n",
    "    print(\"Mean Balanced Accuracy:\", np.round(np.mean(scores), 3))\n",
    "\n",
    "    return scores\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "49287932-11e8-4acd-9b1f-de4baf26c125",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Balanced Accuracy per fold: [0.895 0.926 0.934 0.867 0.871]\n",
      "Mean Balanced Accuracy: 0.899\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([0.89527587, 0.92638538, 0.93438428, 0.86730358, 0.87113002])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate_with_smote_cv(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "214d0595-bf3f-42c7-a626-895c4c717cfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "# import pandas as pd\n",
    "# from sklearn.model_selection import StratifiedKFold, cross_val_score\n",
    "# from sklearn.ensemble import RandomForestClassifier\n",
    "# from sklearn.metrics import make_scorer, balanced_accuracy_score\n",
    "# from imblearn.over_sampling import SMOTE\n",
    "# from imblearn.pipeline import Pipeline\n",
    "# import optuna\n",
    "\n",
    "# # --- Objective function for Bayesian optimization ---\n",
    "# def objective(trial, X, y, n_splits=5, random_state=42):\n",
    "#     # Suggest hyperparameters for Random Forest\n",
    "#     n_estimators = trial.suggest_int('n_estimators', 100, 1000)\n",
    "#     max_depth = trial.suggest_int('max_depth', 3, 30)\n",
    "#     min_samples_split = trial.suggest_int('min_samples_split', 2, 20)\n",
    "#     min_samples_leaf = trial.suggest_int('min_samples_leaf', 1, 10)\n",
    "#     max_features = trial.suggest_categorical('max_features', ['sqrt', 'log2', None])\n",
    "#     bootstrap = trial.suggest_categorical('bootstrap', [True, False])\n",
    "\n",
    "#     # Pipeline with SMOTE + RF\n",
    "#     pipeline = Pipeline(steps=[\n",
    "#         ('smote', SMOTE(random_state=random_state)),\n",
    "#         ('rf', RandomForestClassifier(\n",
    "#             n_estimators=n_estimators,\n",
    "#             max_depth=max_depth,\n",
    "#             min_samples_split=min_samples_split,\n",
    "#             min_samples_leaf=min_samples_leaf,\n",
    "#             max_features=max_features,\n",
    "#             bootstrap=bootstrap,\n",
    "#             class_weight='balanced',\n",
    "#             random_state=random_state,\n",
    "#             n_jobs=-1\n",
    "#         ))\n",
    "#     ])\n",
    "\n",
    "#     # Cross-validation\n",
    "#     skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=random_state)\n",
    "#     scorer = make_scorer(balanced_accuracy_score)\n",
    "#     scores = cross_val_score(pipeline, X, y, cv=skf, scoring=scorer, n_jobs=-1)\n",
    "\n",
    "#     return np.mean(scores)\n",
    "\n",
    "# # --- Run Bayesian optimization ---\n",
    "# def optimize_rf(X, y, n_trials=50, random_state=42):\n",
    "#     study = optuna.create_study(direction=\"maximize\", sampler=optuna.samplers.TPESampler(seed=random_state))\n",
    "#     study.optimize(lambda trial: objective(trial, X, y, random_state=random_state), n_trials=n_trials)\n",
    "\n",
    "#     print(\"Best Trial:\")\n",
    "#     print(\"  Balanced Accuracy:\", study.best_value)\n",
    "#     print(\"  Params:\", study.best_params)\n",
    "\n",
    "#     return study\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "14f8f497-9854-41e2-b5fb-362b399b434a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# study = optimize_rf(X, y, n_trials=50)  # try 50 trials first\n",
    "# best_params = study.best_params\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b1e28716-8b55-4b90-8405-bab33a6b31b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import StratifiedKFold, cross_val_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import make_scorer, balanced_accuracy_score\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.pipeline import Pipeline\n",
    "\n",
    "# --- Updated function with best RF parameters ---\n",
    "def evaluate_with_smote_cv_rfoptimal(X, y, n_splits=5, random_state=42):\n",
    "    \"\"\"\n",
    "    Evaluates a tuned RandomForest model with SMOTE oversampling applied inside CV folds.\n",
    "    Returns balanced accuracy scores for each fold.\n",
    "    \"\"\"\n",
    "\n",
    "    # Define pipeline: SMOTE applied only to training folds\n",
    "    pipeline = Pipeline(steps=[\n",
    "        ('smote', SMOTE(random_state=random_state)),\n",
    "        ('rf', RandomForestClassifier(\n",
    "            n_estimators=437,\n",
    "            max_depth=29,\n",
    "            min_samples_split=15,\n",
    "            min_samples_leaf=6,\n",
    "            max_features='sqrt',\n",
    "            bootstrap=True,\n",
    "            class_weight='balanced',\n",
    "            random_state=random_state,\n",
    "            n_jobs=-1\n",
    "        ))\n",
    "    ])\n",
    "\n",
    "    # Stratified k-fold split\n",
    "    skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=random_state)\n",
    "\n",
    "    # Balanced accuracy scorer\n",
    "    scorer = make_scorer(balanced_accuracy_score)\n",
    "\n",
    "    # Cross-validation with SMOTE inside each training fold\n",
    "    scores = cross_val_score(pipeline, X, y, cv=skf, scoring=scorer, n_jobs=-1)\n",
    "\n",
    "    print(\"Balanced Accuracy per fold:\", np.round(scores, 3))\n",
    "    print(\"Mean Balanced Accuracy:\", np.round(np.mean(scores), 3))\n",
    "\n",
    "    return scores\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4a0f9534-0854-46ab-995c-a1205ae9799f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Balanced Accuracy per fold: [0.919 0.927 0.934 0.885 0.862]\n",
      "Mean Balanced Accuracy: 0.906\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([0.91929318, 0.92696924, 0.93395497, 0.88502762, 0.86230283])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate_with_smote_cv_rfoptimal(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2324d442-c652-4429-8859-7c775fbdbad5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import StratifiedKFold, cross_val_score, train_test_split\n",
    "from sklearn.metrics import make_scorer, balanced_accuracy_score\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.pipeline import Pipeline\n",
    "from lightgbm import LGBMClassifier\n",
    "\n",
    "def evaluate_with_smote_cv_lightgbm(X, y, test_size=0.2, n_splits=5, random_state=42):\n",
    "    \"\"\"\n",
    "    Splits data, evaluates LightGBM with SMOTE + Stratified K-Fold CV,\n",
    "    and trains a final model on the balanced training set.\n",
    "\n",
    "    Args:\n",
    "        X (pd.DataFrame): Feature matrix\n",
    "        y (pd.Series): Target column\n",
    "        test_size (float): Proportion for test split\n",
    "        n_splits (int): Number of folds for cross-validation\n",
    "        random_state (int): Random seed\n",
    "\n",
    "    Returns:\n",
    "        model (LGBMClassifier): Trained LightGBM model on SMOTE-balanced training data\n",
    "        scores (list): Balanced accuracy scores across CV folds\n",
    "        (X_train, X_test, y_train, y_test): Data splits\n",
    "    \"\"\"\n",
    "\n",
    "    # --- 1. Train/test split with stratification ---\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y,\n",
    "        test_size=test_size,\n",
    "        stratify=y,\n",
    "        random_state=random_state\n",
    "    )\n",
    "\n",
    "    # --- 2. Define pipeline: SMOTE + LightGBM ---\n",
    "    pipeline = Pipeline(steps=[\n",
    "        ('smote', SMOTE(random_state=random_state)),\n",
    "        ('lgbm', LGBMClassifier(\n",
    "            n_estimators=500,\n",
    "            learning_rate=0.05,\n",
    "            max_depth=-1,\n",
    "            class_weight='balanced',\n",
    "            random_state=random_state,\n",
    "            n_jobs=-1,\n",
    "            verbose=-1\n",
    "        ))\n",
    "    ])\n",
    "\n",
    "    # --- 3. Stratified CV evaluation ---\n",
    "    skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=random_state)\n",
    "    scorer = make_scorer(balanced_accuracy_score)\n",
    "    scores = cross_val_score(pipeline, X_train, y_train, cv=skf, scoring=scorer, n_jobs=-1)\n",
    "\n",
    "    print(\"Balanced Accuracy per fold:\", np.round(scores, 3))\n",
    "    print(\"Mean Balanced Accuracy:\", np.round(np.mean(scores), 3))\n",
    "\n",
    "    # --- 4. Train final model with SMOTE applied on training set ---\n",
    "    smote = SMOTE(random_state=random_state)\n",
    "    X_train_bal, y_train_bal = smote.fit_resample(X_train, y_train)\n",
    "\n",
    "    model = LGBMClassifier(\n",
    "        n_estimators=500,\n",
    "        learning_rate=0.05,\n",
    "        max_depth=-1,\n",
    "        class_weight='balanced',\n",
    "        random_state=random_state,\n",
    "        n_jobs=-1,\n",
    "        verbose=-1\n",
    "    )\n",
    "\n",
    "    model.fit(X_train_bal, y_train_bal)\n",
    "\n",
    "    return model, scores, (X_train, X_test, y_train, y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7ee22b00-4127-405a-9aee-c2803ba805a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Run evaluation + final training\n",
    "# model, scores, (X_train, X_test, y_train, y_test) = evaluate_with_smote_cv_lightgbm(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "45487fde-7e21-4071-bb0b-ab1a3c329b4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "# import optuna\n",
    "# from sklearn.model_selection import StratifiedKFold, cross_val_score\n",
    "# from sklearn.metrics import make_scorer, balanced_accuracy_score\n",
    "# from imblearn.pipeline import Pipeline\n",
    "# from imblearn.over_sampling import SMOTE\n",
    "# from lightgbm import LGBMClassifier\n",
    "\n",
    "# def objective(trial, X, y, n_splits=5, random_state=42):\n",
    "#     params = {\n",
    "#         \"objective\": \"multiclass\",\n",
    "#         \"num_class\": 3,\n",
    "#         \"n_estimators\": 800,  # fixed during search\n",
    "#         \"learning_rate\": trial.suggest_float(\"learning_rate\", 0.01, 0.15),\n",
    "#         \"num_leaves\": trial.suggest_int(\"num_leaves\", 16, 256, log=True),\n",
    "#         \"max_depth\": trial.suggest_categorical(\"max_depth\", [-1, 4, 6, 8, 10, 12, 16]),\n",
    "#         \"min_child_samples\": trial.suggest_int(\"min_child_samples\", 5, 60),\n",
    "#         \"min_split_gain\": trial.suggest_float(\"min_split_gain\", 0.0, 0.2),\n",
    "#         \"lambda_l1\": trial.suggest_float(\"lambda_l1\", 0.0, 1.0),\n",
    "#         \"lambda_l2\": trial.suggest_float(\"lambda_l2\", 0.0, 2.0),\n",
    "#         \"feature_fraction\": trial.suggest_float(\"feature_fraction\", 0.6, 1.0),\n",
    "#         \"bagging_fraction\": trial.suggest_float(\"bagging_fraction\", 0.6, 1.0),\n",
    "#         \"bagging_freq\": trial.suggest_int(\"bagging_freq\", 1, 7),\n",
    "#         \"extra_trees\": trial.suggest_categorical(\"extra_trees\", [True, False]),\n",
    "#         \"random_state\": random_state,\n",
    "#         \"n_jobs\": -1,\n",
    "#         \"verbose\": -1,\n",
    "#     }\n",
    "\n",
    "#     pipe = Pipeline(steps=[\n",
    "#         (\"smote\", SMOTE(random_state=random_state)),\n",
    "#         (\"lgbm\", LGBMClassifier(**params))\n",
    "#     ])\n",
    "\n",
    "#     skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=random_state)\n",
    "#     scorer = make_scorer(balanced_accuracy_score)\n",
    "#     scores = cross_val_score(pipe, X, y, cv=skf, scoring=scorer, n_jobs=-1)\n",
    "#     return scores.mean()\n",
    "\n",
    "# # Run study\n",
    "# study = optuna.create_study(direction=\"maximize\")\n",
    "# study.optimize(lambda t: objective(t, X, y), n_trials=40, show_progress_bar=True)\n",
    "# best_params = study.best_trial.params\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4d6fc979-8821-4e24-9e7e-ad2a82185355",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "# import pandas as pd\n",
    "# from sklearn.model_selection import StratifiedKFold, cross_val_score, train_test_split\n",
    "# from sklearn.metrics import make_scorer, balanced_accuracy_score\n",
    "# from imblearn.over_sampling import SMOTE\n",
    "# from imblearn.pipeline import Pipeline\n",
    "# from lightgbm import LGBMClassifier\n",
    "\n",
    "# def evaluate_with_smote_cv_lightgbm_optimal(X, y, test_size=0.2, n_splits=5, random_state=42):\n",
    "#     \"\"\"\n",
    "#     Splits data, evaluates LightGBM with SMOTE + Stratified K-Fold CV,\n",
    "#     and trains a final model on the balanced training set using the best tuned parameters.\n",
    "\n",
    "#     Args:\n",
    "#         X (pd.DataFrame): Feature matrix\n",
    "#         y (pd.Series): Target column\n",
    "#         test_size (float): Proportion for test split\n",
    "#         n_splits (int): Number of folds for cross-validation\n",
    "#         random_state (int): Random seed\n",
    "\n",
    "#     Returns:\n",
    "#         model (LGBMClassifier): Trained LightGBM model on SMOTE-balanced training data\n",
    "#         scores (list): Balanced accuracy scores across CV folds\n",
    "#         (X_train, X_test, y_train, y_test): Data splits\n",
    "#     \"\"\"\n",
    "\n",
    "#     # --- 1. Train/test split with stratification ---\n",
    "#     X_train, X_test, y_train, y_test = train_test_split(\n",
    "#         X, y,\n",
    "#         test_size=test_size,\n",
    "#         stratify=y,\n",
    "#         random_state=random_state\n",
    "#     )\n",
    "\n",
    "#     # --- 2. Best parameters from Optuna (Trial 27) ---\n",
    "#     best_params = {\n",
    "#         'learning_rate': 0.09518867841433679,\n",
    "#         'num_leaves': 27,\n",
    "#         'max_depth': 12,\n",
    "#         'min_child_samples': 26,\n",
    "#         'min_split_gain': 0.0637337805202833,\n",
    "#         'lambda_l1': 0.6089094114371527,\n",
    "#         'lambda_l2': 1.2021507701921763,\n",
    "#         'feature_fraction': 0.8839452483534593,\n",
    "#         'bagging_fraction': 0.8166691612094572,\n",
    "#         'bagging_freq': 6,\n",
    "#         'extra_trees': False,\n",
    "#         'n_estimators': 800,           # still use tuned iterations\n",
    "#         'objective': 'multiclass',\n",
    "#         'num_class': 3,\n",
    "#         'random_state': random_state,\n",
    "#         'n_jobs': -1,\n",
    "#         'verbose': -1\n",
    "#     }\n",
    "\n",
    "#     # --- 3. Stratified CV evaluation with SMOTE ---\n",
    "#     pipeline = Pipeline(steps=[\n",
    "#         ('smote', SMOTE(random_state=random_state)),\n",
    "#         ('lgbm', LGBMClassifier(**best_params))\n",
    "#     ])\n",
    "\n",
    "#     skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=random_state)\n",
    "#     scorer = make_scorer(balanced_accuracy_score)\n",
    "#     scores = cross_val_score(pipeline, X_train, y_train, cv=skf, scoring=scorer, n_jobs=-1)\n",
    "\n",
    "#     print(\"Balanced Accuracy per fold:\", np.round(scores, 3))\n",
    "#     print(\"Mean Balanced Accuracy:\", np.round(np.mean(scores), 3))\n",
    "\n",
    "#     # --- 4. Train final model with SMOTE applied on training set ---\n",
    "#     smote = SMOTE(random_state=random_state)\n",
    "#     X_train_bal, y_train_bal = smote.fit_resample(X_train, y_train)\n",
    "\n",
    "#     model = LGBMClassifier(**best_params)\n",
    "#     model.fit(X_train_bal, y_train_bal)\n",
    "\n",
    "#     return model, scores, (X_train, X_test, y_train, y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0d0ee803-14fe-464b-a15d-6bd05dee1bbb",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'evaluate_with_smote_cv_lightgbm_optimal' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Run evaluation + final training\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m model, scores, (X_train, X_test, y_train, y_test) \u001b[38;5;241m=\u001b[39m evaluate_with_smote_cv_lightgbm_optimal(X, y)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'evaluate_with_smote_cv_lightgbm_optimal' is not defined"
     ]
    }
   ],
   "source": [
    "# Run evaluation + final training\n",
    "model, scores, (X_train, X_test, y_train, y_test) = evaluate_with_smote_cv_lightgbm_optimal(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "489014f8-7d75-4fe8-ae24-1c4d26646ee4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def get_feature_importance(model, feature_names, top_n=20):\n",
    "    \"\"\"\n",
    "    Extracts and plots feature importance from a trained LightGBM model.\n",
    "\n",
    "    Args:\n",
    "        model (LGBMClassifier): Trained LightGBM model\n",
    "        feature_names (list): List of feature names\n",
    "        top_n (int): Number of top features to display (default=20)\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: Feature importance sorted by importance\n",
    "    \"\"\"\n",
    "    # Get feature importances (gain = how much each feature contributes to splits)\n",
    "    importance = model.feature_importances_\n",
    "\n",
    "    # Build DataFrame\n",
    "    fi_df = pd.DataFrame({\n",
    "        \"Feature\": feature_names,\n",
    "        \"Importance\": importance\n",
    "    }).sort_values(by=\"Importance\", ascending=False)\n",
    "\n",
    "    # Plot top features\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.barh(fi_df[\"Feature\"].head(top_n)[::-1], fi_df[\"Importance\"].head(top_n)[::-1])\n",
    "    plt.xlabel(\"Importance (split count)\")\n",
    "    plt.ylabel(\"Feature\")\n",
    "    plt.title(\"LightGBM Feature Importance\")\n",
    "    plt.show()\n",
    "\n",
    "    return fi_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f5fe2dd-99c2-4e73-a0a7-7bfba5dfbfa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Get feature importance\n",
    "fi = get_feature_importance(model, X.columns, top_n=20)\n",
    "\n",
    "print(fi.head(20))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fab76bf3-da0c-44fc-b4b9-ed03cfd141fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import shap\n",
    "# import matplotlib.pyplot as plt\n",
    "# import numpy as np\n",
    "\n",
    "# def explain_with_shap_lightgbm(model, X, max_display=20, show_force_plot=False, sample_index=0):\n",
    "#     \"\"\"\n",
    "#     Generates SHAP feature importance analysis for a trained LightGBM model.\n",
    "    \n",
    "#     Args:\n",
    "#         model (LGBMClassifier): Trained LightGBM model\n",
    "#         X (pd.DataFrame): Feature dataframe used during training\n",
    "#         max_display (int): Max number of features to display in SHAP plots\n",
    "#         show_force_plot (bool): Whether to show individual force plot (optional)\n",
    "#         sample_index (int): Row index for local explanation (default: 0)\n",
    "    \n",
    "#     Returns:\n",
    "#         shap_values (list[np.ndarray]): SHAP values for each class\n",
    "#     \"\"\"\n",
    "\n",
    "#     # --- 1. Initialize SHAP Explainer ---\n",
    "#     print(\"Initializing SHAP TreeExplainer...\")\n",
    "#     explainer = shap.TreeExplainer(model)\n",
    "\n",
    "#     # --- 2. Compute SHAP Values ---\n",
    "#     print(\"Computing SHAP values...\")\n",
    "#     shap_values = explainer.shap_values(X)\n",
    "\n",
    "#     # --- 3. Global Interpretability ---\n",
    "#     print(\"\\n=== GLOBAL FEATURE IMPORTANCE ===\")\n",
    "#     plt.title(\"Mean |SHAP value| (Global Feature Importance)\")\n",
    "#     shap.summary_plot(shap_values, X, plot_type=\"bar\", max_display=max_display)\n",
    "\n",
    "#     print(\"\\n=== DETAILED DISTRIBUTION (Beeswarm Plot) ===\")\n",
    "#     shap.summary_plot(shap_values, X, max_display=max_display)\n",
    "\n",
    "#     # --- 4. Local Interpretability (Optional) ---\n",
    "#     if show_force_plot:\n",
    "#         print(f\"\\n=== LOCAL INTERPRETATION for sample index {sample_index} ===\")\n",
    "#         shap.initjs()\n",
    "#         display(shap.force_plot(\n",
    "#             explainer.expected_value[np.argmax(model.predict_proba(X.iloc[[sample_index]])[0])],\n",
    "#             shap_values[np.argmax(model.predict_proba(X.iloc[[sample_index]])[0])][sample_index],\n",
    "#             X.iloc[[sample_index]]\n",
    "#         ))\n",
    "\n",
    "#     # --- 5. Return SHAP values for further custom analysis ---\n",
    "#     return shap_values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9ba27be-c15b-4b38-8ce9-8e9a5a7fbc00",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Generate SHAP explanations on the training data\n",
    "shap_values = explain_with_shap_lightgbm(model, X_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a0c0faef-f9f3-461a-8f04-f054ca2a2996",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nresults = evaluate_with_smote_cv_lightgbm_optimal(\\n    X, y,\\n    test_size=0.2,\\n    n_splits=5,\\n    random_state=42,\\n    use_early_stopping=True,\\n    early_stopping_rounds=50,\\n    use_class_weights=False,\\n    verbose=True\\n)\\n\\n# Access components\\nmodel = results['model']\\ncv_scores = results['cv_scores']\\nfeature_importance = results['feature_importance']\\nX_train, X_test = results['splits']['X_train'], results['splits']['X_test']\\ny_train, y_test = results['splits']['y_train'], results['splits']['y_test']\\n\""
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import StratifiedKFold, cross_val_score, train_test_split\n",
    "from sklearn.metrics import make_scorer, balanced_accuracy_score, classification_report\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.pipeline import Pipeline\n",
    "from lightgbm import LGBMClassifier\n",
    "import lightgbm as lgb\n",
    "\n",
    "\n",
    "def evaluate_with_smote_cv_lightgbm_optimal(\n",
    "    X, y, \n",
    "    test_size=0.2, \n",
    "    n_splits=5, \n",
    "    random_state=42,\n",
    "    use_early_stopping=True,\n",
    "    early_stopping_rounds=50,\n",
    "    use_class_weights=False,\n",
    "    smote_strategy='auto',\n",
    "    verbose=True\n",
    "):\n",
    "    \"\"\"\n",
    "    Enhanced LightGBM training with SMOTE + Stratified K-Fold CV.\n",
    "    \n",
    "    Args:\n",
    "        X (pd.DataFrame): Feature matrix\n",
    "        y (pd.Series): Target column\n",
    "        test_size (float): Proportion for test split\n",
    "        n_splits (int): Number of folds for cross-validation\n",
    "        random_state (int): Random seed\n",
    "        use_early_stopping (bool): Whether to use early stopping for final model\n",
    "        early_stopping_rounds (int): Number of rounds for early stopping\n",
    "        use_class_weights (bool): Use class_weight='balanced' instead of/with SMOTE\n",
    "        smote_strategy (str): SMOTE sampling strategy ('auto', 'not majority', 'all')\n",
    "        verbose (bool): Print detailed output\n",
    "    \n",
    "    Returns:\n",
    "        dict: Contains model, scores, splits, and diagnostics\n",
    "    \"\"\"\n",
    "    # --- 1. Train/test split with stratification ---\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y,\n",
    "        test_size=test_size,\n",
    "        stratify=y,\n",
    "        random_state=random_state\n",
    "    )\n",
    "    \n",
    "    if verbose:\n",
    "        print(\"=\" * 60)\n",
    "        print(\"TRAINING CONFIGURATION\")\n",
    "        print(\"=\" * 60)\n",
    "        print(f\"Train size: {len(X_train)}, Test size: {len(X_test)}\")\n",
    "        print(f\"Class distribution (train): {dict(y_train.value_counts().sort_index())}\")\n",
    "        print(f\"Class distribution (test): {dict(y_test.value_counts().sort_index())}\")\n",
    "        print()\n",
    "    \n",
    "    # --- 2. Best parameters from Optuna (Trial 27) ---\n",
    "    best_params = {\n",
    "        'learning_rate': 0.09518867841433679,\n",
    "        'num_leaves': 27,\n",
    "        'max_depth': 12,\n",
    "        'min_child_samples': 26,\n",
    "        'min_split_gain': 0.0637337805202833,\n",
    "        'lambda_l1': 0.6089094114371527,\n",
    "        'lambda_l2': 1.2021507701921763,\n",
    "        'feature_fraction': 0.8839452483534593,\n",
    "        'bagging_fraction': 0.8166691612094572,\n",
    "        'bagging_freq': 6,\n",
    "        'extra_trees': False,\n",
    "        'n_estimators': 800,\n",
    "        'objective': 'multiclass',\n",
    "        'num_class': 3,\n",
    "        'random_state': random_state,\n",
    "        'n_jobs': -1,\n",
    "        'verbose': -1\n",
    "    }\n",
    "    \n",
    "    # Add class weights if requested\n",
    "    if use_class_weights:\n",
    "        best_params['class_weight'] = 'balanced'\n",
    "        if verbose:\n",
    "            print(\"Using class_weight='balanced'\")\n",
    "    \n",
    "    # --- 3. Stratified CV evaluation with SMOTE ---\n",
    "    if verbose:\n",
    "        print(\"=\" * 60)\n",
    "        print(\"CROSS-VALIDATION EVALUATION\")\n",
    "        print(\"=\" * 60)\n",
    "    \n",
    "    pipeline = Pipeline(steps=[\n",
    "        ('smote', SMOTE(random_state=random_state, sampling_strategy=smote_strategy)),\n",
    "        ('lgbm', LGBMClassifier(**best_params))\n",
    "    ])\n",
    "    \n",
    "    skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=random_state)\n",
    "    scorer = make_scorer(balanced_accuracy_score)\n",
    "    scores = cross_val_score(pipeline, X_train, y_train, cv=skf, scoring=scorer, n_jobs=-1)\n",
    "    \n",
    "    if verbose:\n",
    "        print(f\"Balanced Accuracy per fold: {np.round(scores, 4)}\")\n",
    "        print(f\"Mean Balanced Accuracy: {np.round(np.mean(scores), 4)} ± {np.round(np.std(scores), 4)}\")\n",
    "        print()\n",
    "    \n",
    "    # --- 4. Train final model with SMOTE applied on training set ---\n",
    "    if verbose:\n",
    "        print(\"=\" * 60)\n",
    "        print(\"FINAL MODEL TRAINING\")\n",
    "        print(\"=\" * 60)\n",
    "    \n",
    "    smote = SMOTE(random_state=random_state, sampling_strategy=smote_strategy)\n",
    "    X_train_bal, y_train_bal = smote.fit_resample(X_train, y_train)\n",
    "    \n",
    "    if verbose:\n",
    "        print(f\"After SMOTE - Train size: {len(X_train_bal)}\")\n",
    "        print(f\"Class distribution (balanced): {dict(pd.Series(y_train_bal).value_counts().sort_index())}\")\n",
    "        print()\n",
    "    \n",
    "    model = LGBMClassifier(**best_params)\n",
    "    \n",
    "    # Fit with or without early stopping\n",
    "    if use_early_stopping:\n",
    "        callbacks = [lgb.early_stopping(early_stopping_rounds, verbose=False)]\n",
    "        model.fit(\n",
    "            X_train_bal, y_train_bal,\n",
    "            eval_set=[(X_test, y_test)],\n",
    "            callbacks=callbacks\n",
    "        )\n",
    "        if verbose and hasattr(model, 'best_iteration_'):\n",
    "            print(f\"Best iteration: {model.best_iteration_} (out of {best_params['n_estimators']})\")\n",
    "    else:\n",
    "        model.fit(X_train_bal, y_train_bal)\n",
    "    \n",
    "    # --- 5. Feature importance ---\n",
    "    feature_importance = pd.DataFrame({\n",
    "        'feature': X_train.columns,\n",
    "        'importance': model.feature_importances_\n",
    "    }).sort_values('importance', ascending=False)\n",
    "    \n",
    "    # --- 6. Test set evaluation ---\n",
    "    y_pred = model.predict(X_test)\n",
    "    test_balanced_acc = balanced_accuracy_score(y_test, y_pred)\n",
    "    \n",
    "    if verbose:\n",
    "        print(f\"\\nTest Set Balanced Accuracy: {np.round(test_balanced_acc, 4)}\")\n",
    "        print(\"\\nClassification Report (Test Set):\")\n",
    "        print(classification_report(y_test, y_pred))\n",
    "        print(\"\\nTop 10 Most Important Features:\")\n",
    "        print(feature_importance.head(10).to_string(index=False))\n",
    "        print(\"=\" * 60)\n",
    "    \n",
    "    # --- 7. Return comprehensive results ---\n",
    "    return {\n",
    "        'model': model,\n",
    "        'cv_scores': scores,\n",
    "        'cv_mean': np.mean(scores),\n",
    "        'cv_std': np.std(scores),\n",
    "        'test_balanced_accuracy': test_balanced_acc,\n",
    "        'feature_importance': feature_importance,\n",
    "        'splits': {\n",
    "            'X_train': X_train,\n",
    "            'X_test': X_test,\n",
    "            'y_train': y_train,\n",
    "            'y_test': y_test\n",
    "        },\n",
    "        'balanced_train_shape': X_train_bal.shape,\n",
    "        'predictions': y_pred,\n",
    "        'hyperparameters': best_params\n",
    "    }\n",
    "\n",
    "\n",
    "# Example usage:\n",
    "\"\"\"\n",
    "results = evaluate_with_smote_cv_lightgbm_optimal(\n",
    "    X, y,\n",
    "    test_size=0.2,\n",
    "    n_splits=5,\n",
    "    random_state=42,\n",
    "    use_early_stopping=True,\n",
    "    early_stopping_rounds=50,\n",
    "    use_class_weights=False,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "# Access components\n",
    "model = results['model']\n",
    "cv_scores = results['cv_scores']\n",
    "feature_importance = results['feature_importance']\n",
    "X_train, X_test = results['splits']['X_train'], results['splits']['X_test']\n",
    "y_train, y_test = results['splits']['y_train'], results['splits']['y_test']\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "39a5cd0d-4343-494b-979b-da8017a2a4d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "TRAINING CONFIGURATION\n",
      "============================================================\n",
      "Train size: 1692, Test size: 423\n",
      "Class distribution (train): {1.0: 1318, 2.0: 234, 3.0: 140}\n",
      "Class distribution (test): {1.0: 329, 2.0: 59, 3.0: 35}\n",
      "\n",
      "============================================================\n",
      "CROSS-VALIDATION EVALUATION\n",
      "============================================================\n",
      "Balanced Accuracy per fold: [0.8706 0.8914 0.9188 0.8916 0.9139]\n",
      "Mean Balanced Accuracy: 0.8972 ± 0.0174\n",
      "\n",
      "============================================================\n",
      "FINAL MODEL TRAINING\n",
      "============================================================\n",
      "After SMOTE - Train size: 3954\n",
      "Class distribution (balanced): {1.0: 1318, 2.0: 1318, 3.0: 1318}\n",
      "\n",
      "Best iteration: 82 (out of 800)\n",
      "\n",
      "Test Set Balanced Accuracy: 0.9261\n",
      "\n",
      "Classification Report (Test Set):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         1.0       0.97      0.99      0.98       329\n",
      "         2.0       0.91      0.85      0.88        59\n",
      "         3.0       1.00      0.94      0.97        35\n",
      "\n",
      "    accuracy                           0.96       423\n",
      "   macro avg       0.96      0.93      0.94       423\n",
      "weighted avg       0.96      0.96      0.96       423\n",
      "\n",
      "\n",
      "Top 10 Most Important Features:\n",
      " feature  importance\n",
      "    ASTV         602\n",
      "    ALTV         457\n",
      "       e         374\n",
      "       b         343\n",
      "Variance         332\n",
      "      LB         325\n",
      "    Mode         295\n",
      "    Nmax         291\n",
      "  Median         284\n",
      "    MLTV         280\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "results = evaluate_with_smote_cv_lightgbm_optimal(\n",
    "    X, y,\n",
    "    test_size=0.2,\n",
    "    n_splits=5,\n",
    "    random_state=42,\n",
    "    use_early_stopping=True,\n",
    "    early_stopping_rounds=50,\n",
    "    use_class_weights=False,\n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "9fb59f45-5954-4d0e-8d23-cdbb505de7a6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n# After training your model with the enhanced function:\\nresults = evaluate_with_smote_cv_lightgbm_optimal(X, y)\\nmodel = results['model']\\nX_train = results['splits']['X_train']\\nX_test = results['splits']['X_test']\\n\\n# Option 1: Quick analysis without interactions (if memory issues)\\nshap_results = shap_feature_analysis(\\n    model=model,\\n    X_train=X_train,\\n    X_test=X_test,\\n    sample_size=200,\\n    calculate_interactions=False,  # Skip interactions to save memory\\n    low_importance_threshold=0.01,\\n    plot=True\\n)\\n\\n# Option 2: Full analysis with interactions (smaller samples)\\nshap_results = shap_feature_analysis(\\n    model=model,\\n    X_train=X_train,\\n    X_test=X_test,\\n    sample_size=200,\\n    interaction_sample_size=50,  # Very small sample for interactions\\n    calculate_interactions=True,\\n    low_importance_threshold=0.01,\\n    plot=True\\n)\\n\\n# Get features to drop\\nfeatures_to_drop = shap_results['features_to_drop_moderate']  # Based on importance only\\n\\n# Retrain model without these features\\nX_reduced = X.drop(columns=features_to_drop)\\nresults_reduced = evaluate_with_smote_cv_lightgbm_optimal(X_reduced, y)\\n\""
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import shap\n",
    "from typing import Optional, Tuple, Dict, List\n",
    "\n",
    "\n",
    "def shap_feature_analysis(\n",
    "    model,\n",
    "    X_train: pd.DataFrame,\n",
    "    X_test: pd.DataFrame,\n",
    "    sample_size: int = 200,\n",
    "    interaction_sample_size: int = 50,\n",
    "    calculate_interactions: bool = True,\n",
    "    low_importance_threshold: float = 0.01,\n",
    "    low_interaction_threshold: float = 0.001,\n",
    "    plot: bool = True,\n",
    "    figsize: Tuple[int, int] = (15, 10)\n",
    ") -> Dict:\n",
    "    \"\"\"\n",
    "    Comprehensive SHAP analysis to identify features for removal based on \n",
    "    global importance and interaction effects.\n",
    "    \n",
    "    Args:\n",
    "        model: Trained LightGBM/tree-based model\n",
    "        X_train: Training feature matrix\n",
    "        X_test: Test feature matrix\n",
    "        sample_size: Number of samples for SHAP value calculation (reduced default)\n",
    "        interaction_sample_size: Number of samples for interaction analysis (reduced default)\n",
    "        calculate_interactions: Whether to calculate interactions (set False if memory issues)\n",
    "        low_importance_threshold: Threshold for low importance (as fraction of max importance)\n",
    "        low_interaction_threshold: Threshold for low interaction (as fraction of max interaction)\n",
    "        plot: Whether to generate visualizations\n",
    "        figsize: Figure size for plots\n",
    "    \n",
    "    Returns:\n",
    "        dict: Comprehensive analysis results including features to drop\n",
    "    \"\"\"\n",
    "    print(\"=\" * 80)\n",
    "    print(\"SHAP FEATURE IMPORTANCE ANALYSIS\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    # --- 1. Initialize SHAP explainer ---\n",
    "    print(\"\\n[1/5] Initializing SHAP explainer...\")\n",
    "    explainer = shap.TreeExplainer(model)\n",
    "    \n",
    "    # Sample data for efficiency\n",
    "    X_train_sample = X_train.sample(min(sample_size, len(X_train)), random_state=42)\n",
    "    X_test_sample = X_test.sample(min(sample_size, len(X_test)), random_state=42)\n",
    "    \n",
    "    # --- 2. Calculate SHAP values ---\n",
    "    print(f\"[2/5] Calculating SHAP values for {len(X_train_sample)} training samples...\")\n",
    "    shap_values_train = explainer(X_train_sample)\n",
    "    \n",
    "    print(f\"      Calculating SHAP values for {len(X_test_sample)} test samples...\")\n",
    "    shap_values_test = explainer(X_test_sample)\n",
    "    \n",
    "    # For multiclass, take mean across classes\n",
    "    if len(shap_values_train.shape) == 3:\n",
    "        shap_values_train_mean = np.abs(shap_values_train.values).mean(axis=(0, 2))\n",
    "        shap_values_test_mean = np.abs(shap_values_test.values).mean(axis=(0, 2))\n",
    "    else:\n",
    "        shap_values_train_mean = np.abs(shap_values_train.values).mean(axis=0)\n",
    "        shap_values_test_mean = np.abs(shap_values_test.values).mean(axis=0)\n",
    "    \n",
    "    # --- 3. Global feature importance ---\n",
    "    print(\"[3/5] Computing global feature importance...\")\n",
    "    \n",
    "    feature_importance = pd.DataFrame({\n",
    "        'feature': X_train.columns,\n",
    "        'importance_train': shap_values_train_mean,\n",
    "        'importance_test': shap_values_test_mean\n",
    "    })\n",
    "    feature_importance['importance_avg'] = (\n",
    "        feature_importance['importance_train'] + feature_importance['importance_test']\n",
    "    ) / 2\n",
    "    feature_importance = feature_importance.sort_values('importance_avg', ascending=False)\n",
    "    \n",
    "    # Normalize to max importance\n",
    "    max_importance = feature_importance['importance_avg'].max()\n",
    "    feature_importance['importance_normalized'] = (\n",
    "        feature_importance['importance_avg'] / max_importance\n",
    "    )\n",
    "    \n",
    "    # Identify low importance features\n",
    "    low_importance_features = feature_importance[\n",
    "        feature_importance['importance_normalized'] < low_importance_threshold\n",
    "    ]['feature'].tolist()\n",
    "    \n",
    "    print(f\"\\n   Found {len(low_importance_features)} low importance features\")\n",
    "    print(f\"   (threshold: {low_importance_threshold * 100}% of max importance)\")\n",
    "    \n",
    "    # --- 4. Feature interactions ---\n",
    "    interaction_df = None\n",
    "    interaction_strength = None\n",
    "    low_interaction_features = []\n",
    "    \n",
    "    if calculate_interactions:\n",
    "        print(f\"\\n[4/5] Calculating feature interactions for top features...\")\n",
    "        print(f\"      (Using {interaction_sample_size} samples - this may take a moment)\")\n",
    "        \n",
    "        try:\n",
    "            # Sample for interaction analysis (computationally expensive)\n",
    "            X_interaction_sample = X_train.sample(\n",
    "                min(interaction_sample_size, len(X_train)), \n",
    "                random_state=42\n",
    "            )\n",
    "            \n",
    "            # Calculate interactions for top 15 features only (reduced from 20)\n",
    "            top_features = feature_importance.head(15)['feature'].tolist()\n",
    "            X_interaction_top = X_interaction_sample[top_features]\n",
    "            \n",
    "            print(f\"      Computing interactions for top {len(top_features)} features...\")\n",
    "            shap_interaction_values = explainer.shap_interaction_values(X_interaction_top)\n",
    "            \n",
    "            # For multiclass, average across classes\n",
    "            if isinstance(shap_interaction_values, list):\n",
    "                shap_interaction_values = np.mean(\n",
    "                    [np.abs(arr) for arr in shap_interaction_values], \n",
    "                    axis=0\n",
    "                )\n",
    "            else:\n",
    "                shap_interaction_values = np.abs(shap_interaction_values)\n",
    "            \n",
    "            # Average interaction matrix\n",
    "            interaction_matrix = np.mean(shap_interaction_values, axis=0)\n",
    "            \n",
    "            # Create interaction dataframe\n",
    "            interaction_df = pd.DataFrame(\n",
    "                interaction_matrix,\n",
    "                index=top_features,\n",
    "                columns=top_features\n",
    "            )\n",
    "            \n",
    "            # Get total interaction strength per feature\n",
    "            # Exclude diagonal (self-interaction)\n",
    "            np.fill_diagonal(interaction_matrix, 0)\n",
    "            interaction_strength = pd.DataFrame({\n",
    "                'feature': top_features,\n",
    "                'total_interaction': interaction_matrix.sum(axis=1)\n",
    "            }).sort_values('total_interaction', ascending=False)\n",
    "            \n",
    "            max_interaction = interaction_strength['total_interaction'].max()\n",
    "            interaction_strength['interaction_normalized'] = (\n",
    "                interaction_strength['total_interaction'] / max_interaction\n",
    "            )\n",
    "            \n",
    "            # Identify features with low interaction\n",
    "            low_interaction_features = interaction_strength[\n",
    "                interaction_strength['interaction_normalized'] < low_interaction_threshold\n",
    "            ]['feature'].tolist()\n",
    "            \n",
    "            print(f\"\\n   Found {len(low_interaction_features)} features with low interactions\")\n",
    "            print(f\"   (threshold: {low_interaction_threshold * 100}% of max interaction)\")\n",
    "            \n",
    "            # Clear memory\n",
    "            del shap_interaction_values\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"\\n   WARNING: Interaction calculation failed: {str(e)}\")\n",
    "            print(\"   Continuing with importance analysis only...\")\n",
    "            calculate_interactions = False\n",
    "    else:\n",
    "        print(\"\\n[4/5] Skipping interaction calculation (calculate_interactions=False)\")\n",
    "    \n",
    "    # --- 5. Recommendations for feature removal ---\n",
    "    print(\"\\n[5/5] Generating recommendations...\")\n",
    "    \n",
    "    # Features that are BOTH low importance AND low interaction (if calculated)\n",
    "    if calculate_interactions and low_interaction_features:\n",
    "        features_to_drop_strict = list(\n",
    "            set(low_importance_features) & set(low_interaction_features)\n",
    "        )\n",
    "    else:\n",
    "        features_to_drop_strict = []\n",
    "    \n",
    "    # Features that are low importance (regardless of interaction)\n",
    "    features_to_drop_moderate = low_importance_features\n",
    "    \n",
    "    # Create comprehensive report\n",
    "    drop_recommendations = pd.DataFrame({\n",
    "        'feature': X_train.columns,\n",
    "        'importance_normalized': feature_importance.set_index('feature').loc[\n",
    "            X_train.columns, 'importance_normalized'\n",
    "        ].values,\n",
    "        'is_low_importance': [f in low_importance_features for f in X_train.columns],\n",
    "        'is_low_interaction': [\n",
    "            f in low_interaction_features if (calculate_interactions and f in interaction_strength['feature'].values) else None \n",
    "            for f in X_train.columns\n",
    "        ]\n",
    "    }).sort_values('importance_normalized', ascending=True)\n",
    "    \n",
    "    # --- 6. Visualizations ---\n",
    "    if plot:\n",
    "        print(\"\\nGenerating visualizations...\")\n",
    "        fig = plt.figure(figsize=figsize)\n",
    "        \n",
    "        # Plot 1: Feature Importance (Top 20)\n",
    "        plt.subplot(2, 2, 1)\n",
    "        top_20 = feature_importance.head(20)\n",
    "        plt.barh(range(len(top_20)), top_20['importance_avg'])\n",
    "        plt.yticks(range(len(top_20)), top_20['feature'])\n",
    "        plt.xlabel('Mean |SHAP value|')\n",
    "        plt.title('Top 20 Features by SHAP Importance')\n",
    "        plt.gca().invert_yaxis()\n",
    "        \n",
    "        # Plot 2: Feature Importance (Bottom 20)\n",
    "        plt.subplot(2, 2, 2)\n",
    "        bottom_20 = feature_importance.tail(20)\n",
    "        colors = ['red' if f in features_to_drop_strict else 'orange' \n",
    "                  if f in features_to_drop_moderate else 'gray' \n",
    "                  for f in bottom_20['feature']]\n",
    "        plt.barh(range(len(bottom_20)), bottom_20['importance_avg'], color=colors)\n",
    "        plt.yticks(range(len(bottom_20)), bottom_20['feature'])\n",
    "        plt.xlabel('Mean |SHAP value|')\n",
    "        plt.title('Bottom 20 Features (Red=Drop Strict, Orange=Drop Moderate)')\n",
    "        plt.axvline(x=max_importance * low_importance_threshold, \n",
    "                   color='red', linestyle='--', alpha=0.5, label='Threshold')\n",
    "        plt.legend()\n",
    "        plt.gca().invert_yaxis()\n",
    "        \n",
    "        # Plot 3: Interaction Heatmap\n",
    "        if calculate_interactions and interaction_df is not None:\n",
    "            plt.subplot(2, 2, 3)\n",
    "            sns.heatmap(\n",
    "                interaction_df.head(10).iloc[:, :10], \n",
    "                cmap='YlOrRd', \n",
    "                cbar_kws={'label': 'Interaction Strength'}\n",
    "            )\n",
    "            plt.title('Feature Interaction Heatmap (Top 10 Features)')\n",
    "            plt.xticks(rotation=45, ha='right')\n",
    "            plt.yticks(rotation=0)\n",
    "        else:\n",
    "            plt.subplot(2, 2, 3)\n",
    "            plt.text(0.5, 0.5, 'Interaction analysis\\nnot calculated', \n",
    "                    ha='center', va='center', fontsize=12)\n",
    "            plt.axis('off')\n",
    "        \n",
    "        # Plot 4: SHAP Summary Plot\n",
    "        plt.subplot(2, 2, 4)\n",
    "        shap.summary_plot(\n",
    "            shap_values_test, \n",
    "            X_test_sample, \n",
    "            max_display=20,\n",
    "            show=False\n",
    "        )\n",
    "        plt.title('SHAP Summary Plot (Test Set)')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    \n",
    "    # --- 7. Print summary ---\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"SUMMARY & RECOMMENDATIONS\")\n",
    "    print(\"=\" * 80)\n",
    "    print(f\"\\nTotal features: {len(X_train.columns)}\")\n",
    "    print(f\"\\nFeatures to DROP (strict - low importance AND low interaction):\")\n",
    "    print(f\"  Count: {len(features_to_drop_strict)}\")\n",
    "    if features_to_drop_strict:\n",
    "        print(f\"  Features: {features_to_drop_strict[:10]}\")\n",
    "        if len(features_to_drop_strict) > 10:\n",
    "            print(f\"  ... and {len(features_to_drop_strict) - 10} more\")\n",
    "    \n",
    "    print(f\"\\nFeatures to CONSIDER dropping (moderate - low importance only):\")\n",
    "    print(f\"  Count: {len(features_to_drop_moderate)}\")\n",
    "    if features_to_drop_moderate:\n",
    "        print(f\"  Features: {features_to_drop_moderate[:10]}\")\n",
    "        if len(features_to_drop_moderate) > 10:\n",
    "            print(f\"  ... and {len(features_to_drop_moderate) - 10} more\")\n",
    "    \n",
    "    print(f\"\\nTop 5 most important features:\")\n",
    "    for idx, row in feature_importance.head(5).iterrows():\n",
    "        print(f\"  {row['feature']}: {row['importance_normalized']:.4f}\")\n",
    "    \n",
    "    if calculate_interactions and interaction_strength is not None:\n",
    "        print(f\"\\nTop 5 features by interaction strength:\")\n",
    "        for idx, row in interaction_strength.head(5).iterrows():\n",
    "            print(f\"  {row['feature']}: {row['interaction_normalized']:.4f}\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    \n",
    "    # --- 8. Return comprehensive results ---\n",
    "    return {\n",
    "        'feature_importance': feature_importance,\n",
    "        'interaction_strength': interaction_strength,\n",
    "        'interaction_matrix': interaction_df,\n",
    "        'drop_recommendations': drop_recommendations,\n",
    "        'features_to_drop_strict': features_to_drop_strict,\n",
    "        'features_to_drop_moderate': features_to_drop_moderate,\n",
    "        'low_importance_features': low_importance_features,\n",
    "        'low_interaction_features': low_interaction_features,\n",
    "        'shap_values_train': shap_values_train,\n",
    "        'shap_values_test': shap_values_test,\n",
    "        'thresholds': {\n",
    "            'importance': low_importance_threshold,\n",
    "            'interaction': low_interaction_threshold\n",
    "        }\n",
    "    }\n",
    "\n",
    "\n",
    "# Example usage:\n",
    "\"\"\"\n",
    "# After training your model with the enhanced function:\n",
    "results = evaluate_with_smote_cv_lightgbm_optimal(X, y)\n",
    "model = results['model']\n",
    "X_train = results['splits']['X_train']\n",
    "X_test = results['splits']['X_test']\n",
    "\n",
    "# Option 1: Quick analysis without interactions (if memory issues)\n",
    "shap_results = shap_feature_analysis(\n",
    "    model=model,\n",
    "    X_train=X_train,\n",
    "    X_test=X_test,\n",
    "    sample_size=200,\n",
    "    calculate_interactions=False,  # Skip interactions to save memory\n",
    "    low_importance_threshold=0.01,\n",
    "    plot=True\n",
    ")\n",
    "\n",
    "# Option 2: Full analysis with interactions (smaller samples)\n",
    "shap_results = shap_feature_analysis(\n",
    "    model=model,\n",
    "    X_train=X_train,\n",
    "    X_test=X_test,\n",
    "    sample_size=200,\n",
    "    interaction_sample_size=50,  # Very small sample for interactions\n",
    "    calculate_interactions=True,\n",
    "    low_importance_threshold=0.01,\n",
    "    plot=True\n",
    ")\n",
    "\n",
    "# Get features to drop\n",
    "features_to_drop = shap_results['features_to_drop_moderate']  # Based on importance only\n",
    "\n",
    "# Retrain model without these features\n",
    "X_reduced = X.drop(columns=features_to_drop)\n",
    "results_reduced = evaluate_with_smote_cv_lightgbm_optimal(X_reduced, y)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f8787dc1-a49f-408a-a547-688e0eb64bb7",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'results' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# After training your model\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# results = evaluate_with_smote_cv_lightgbm_optimal(X, y)\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m model \u001b[38;5;241m=\u001b[39m results[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m      4\u001b[0m X_train \u001b[38;5;241m=\u001b[39m results[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msplits\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mX_train\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m      5\u001b[0m X_test \u001b[38;5;241m=\u001b[39m results[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msplits\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mX_test\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "\u001b[0;31mNameError\u001b[0m: name 'results' is not defined"
     ]
    }
   ],
   "source": [
    "# After training your model\n",
    "# results = evaluate_with_smote_cv_lightgbm_optimal(X, y)\n",
    "model = results['model']\n",
    "X_train = results['splits']['X_train']\n",
    "X_test = results['splits']['X_test']\n",
    "\n",
    "# Run SHAP analysis\n",
    "shap_results = shap_feature_analysis(\n",
    "    model=model,\n",
    "    X_train=X_train,\n",
    "    X_test=X_test,\n",
    "    low_importance_threshold=0.01,  # Features with <1% of max importance\n",
    "    plot=True\n",
    ")\n",
    "\n",
    "# Get recommended features to drop\n",
    "features_to_drop = shap_results['features_to_drop_strict']\n",
    "\n",
    "# Retrain without these features\n",
    "X_reduced = X.drop(columns=features_to_drop)\n",
    "results_new = evaluate_with_smote_cv_lightgbm_optimal(X_reduced, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d2f2219b-d88d-45bd-9847-aae9932a8c50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lol\n"
     ]
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
