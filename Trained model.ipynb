{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f860ae41-e92d-41ee-a95d-d96509ef0d0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Core libraries and model APIs\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold, cross_val_score\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import balanced_accuracy_score, f1_score, classification_report\n",
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "from sklearn.utils.class_weight import compute_class_weight, compute_sample_weight\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "69749016-4557-42a4-90ae-9732d46f8603",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final dataset: 2098 samples, 8 features\n"
     ]
    }
   ],
   "source": [
    "def clean_data(df: pd.DataFrame):\n",
    "    \"\"\"\n",
    "    Cleans the input dataframe by:\n",
    "    1. Removing empty rows and columns.\n",
    "    2. Keeping only the specified features and the target.\n",
    "    3. Removing duplicate rows.\n",
    "    4. Dropping rows with missing target values.\n",
    "    5. Splitting into features (X) and target (y).\n",
    "    \n",
    "    Args:\n",
    "        df (pd.DataFrame): The raw dataframe.\n",
    "    \n",
    "    Returns:\n",
    "        tuple: (X, y) where\n",
    "            X (pd.DataFrame): Feature dataframe (restricted to chosen features).\n",
    "            y (pd.Series): Target column ('NSP').\n",
    "    \"\"\"\n",
    "    # Step 1: Make a working copy of the dataframe\n",
    "    cleaned = df.copy()\n",
    "    \n",
    "    # Step 2: Drop any rows or columns that are completely empty\n",
    "    cleaned = cleaned.dropna(axis=0, how='all').dropna(axis=1, how='all')\n",
    "    \n",
    "    # Step 3: Define target and allowed features\n",
    "    target_col = 'NSP'\n",
    "    allowed_features = {'ASTV', 'DP', 'ALTV', 'Median', 'Variance', 'AC', 'UC', 'Mode'}\n",
    "    \n",
    "    # Step 4: Keep only allowed features + target (with validation)\n",
    "    available_features = [c for c in allowed_features if c in cleaned.columns]\n",
    "    \n",
    "    # Warn if target column is missing\n",
    "    if target_col not in cleaned.columns:\n",
    "        raise ValueError(f\"Target column '{target_col}' not found in dataframe\")\n",
    "    \n",
    "    # Warn about missing features (optional but helpful)\n",
    "    missing_features = allowed_features - set(available_features)\n",
    "    if missing_features:\n",
    "        print(f\"Warning: The following features are missing: {missing_features}\")\n",
    "    \n",
    "    keep_cols = available_features + [target_col]\n",
    "    cleaned = cleaned[keep_cols]\n",
    "    \n",
    "    # Step 5: Remove duplicate rows\n",
    "    cleaned = cleaned.drop_duplicates()\n",
    "    \n",
    "    # Step 6: Drop rows where target is missing\n",
    "    cleaned = cleaned.dropna(subset=[target_col])\n",
    "    \n",
    "    # Step 7: Handle missing values in features (optional - you might want to impute)\n",
    "    # For now, dropping rows with any missing feature values\n",
    "    initial_rows = len(cleaned)\n",
    "    cleaned = cleaned.dropna()\n",
    "    dropped_rows = initial_rows - len(cleaned)\n",
    "    if dropped_rows > 0:\n",
    "        print(f\"Dropped {dropped_rows} rows with missing feature values\")\n",
    "    \n",
    "    # Step 8: Separate features and target\n",
    "    X = cleaned.drop(columns=[target_col])\n",
    "    y = cleaned[target_col]\n",
    "\n",
    "    # Add at the end before returning:\n",
    "    # Reset index to avoid gaps after dropping rows\n",
    "    X = X.reset_index(drop=True)\n",
    "    y = y.reset_index(drop=True)\n",
    "    \n",
    "    print(f\"Final dataset: {len(X)} samples, {len(X.columns)} features\")\n",
    "    \n",
    "    return X, y\n",
    "\n",
    "\n",
    "df = pd.read_excel('ctg.xlsx', sheet_name = 'Data', header = 1)\n",
    "df\n",
    "\n",
    "X, y = clean_data(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2da5c8e8-5d23-4cf8-9fa8-e65c90be0725",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nmodel, X_train, X_test, y_train, y_test, cv_scores, test_balanced_accuracy, feature_importance, hyperparameters, predictions = evaluate_with_smote_cv_lightgbm_optimal2(\\n    X, y,\\n    test_size=0.2,\\n    n_splits=5,\\n    random_state=42,\\n    use_early_stopping=True,\\n    early_stopping_rounds=50,\\n    use_class_weights=False,\\n    verbose=True\\n)\\n\\n# Now you can use variables directly\\nprint(f\"Model: {model}\")\\nprint(f\"CV Scores: {cv_scores}\")\\nprint(f\"Test Accuracy: {test_balanced_accuracy}\")\\n'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import StratifiedKFold, cross_val_score, train_test_split\n",
    "from sklearn.metrics import make_scorer, balanced_accuracy_score, classification_report\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.pipeline import Pipeline\n",
    "from lightgbm import LGBMClassifier\n",
    "import lightgbm as lgb\n",
    "\n",
    "\n",
    "def evaluate_with_smote_cv_lightgbm_optimal2(\n",
    "    X, y, \n",
    "    test_size=0.2, \n",
    "    n_splits=5, \n",
    "    random_state=42,\n",
    "    use_early_stopping=True,\n",
    "    early_stopping_rounds=50,\n",
    "    use_class_weights=False,\n",
    "    smote_strategy='auto',\n",
    "    verbose=True\n",
    "):\n",
    "    \"\"\"\n",
    "    Enhanced LightGBM training with SMOTE + Stratified K-Fold CV.\n",
    "    \n",
    "    Args:\n",
    "        X (pd.DataFrame): Feature matrix\n",
    "        y (pd.Series): Target column\n",
    "        test_size (float): Proportion for test split\n",
    "        n_splits (int): Number of folds for cross-validation\n",
    "        random_state (int): Random seed\n",
    "        use_early_stopping (bool): Whether to use early stopping for final model\n",
    "        early_stopping_rounds (int): Number of rounds for early stopping\n",
    "        use_class_weights (bool): Use class_weight='balanced' instead of/with SMOTE\n",
    "        smote_strategy (str): SMOTE sampling strategy ('auto', 'not majority', 'all')\n",
    "        verbose (bool): Print detailed output\n",
    "    \n",
    "    Returns:\n",
    "        tuple: (model, X_train, X_test, y_train, y_test, cv_scores, \n",
    "                test_balanced_accuracy, feature_importance, hyperparameters, predictions)\n",
    "    \"\"\"\n",
    "    # --- 1. Train/test split with stratification ---\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y,\n",
    "        test_size=test_size,\n",
    "        stratify=y,\n",
    "        random_state=random_state\n",
    "    )\n",
    "    \n",
    "    if verbose:\n",
    "        print(\"=\" * 60)\n",
    "        print(\"TRAINING CONFIGURATION\")\n",
    "        print(\"=\" * 60)\n",
    "        print(f\"Train size: {len(X_train)}, Test size: {len(X_test)}\")\n",
    "        print(f\"Class distribution (train): {dict(y_train.value_counts().sort_index())}\")\n",
    "        print(f\"Class distribution (test): {dict(y_test.value_counts().sort_index())}\")\n",
    "        print()\n",
    "    \n",
    "    # --- 2. Best parameters from Optuna (Updated) ---\n",
    "    best_params = {\n",
    "        'learning_rate': 0.015148189601668145,\n",
    "        'num_leaves': 24,\n",
    "        'max_depth': 3,\n",
    "        'min_child_samples': 25,\n",
    "        'min_split_gain': 0.44871424998167664,\n",
    "        'lambda_l1': 0.799084665275944,\n",
    "        'lambda_l2': 0.7644980529191521,\n",
    "        'feature_fraction': 0.9596036089327271,\n",
    "        'bagging_fraction': 0.9098171071119672,\n",
    "        'bagging_freq': 1,\n",
    "        'extra_trees': False,\n",
    "        'n_estimators': 295,\n",
    "        'objective': 'multiclass',\n",
    "        'num_class': 3,\n",
    "        'random_state': random_state,\n",
    "        'n_jobs': -1,\n",
    "        'verbose': -1\n",
    "    }\n",
    "    \n",
    "    # Add class weights if requested\n",
    "    if use_class_weights:\n",
    "        best_params['class_weight'] = 'balanced'\n",
    "        if verbose:\n",
    "            print(\"Using class_weight='balanced'\")\n",
    "    \n",
    "    # --- 3. Stratified CV evaluation with SMOTE ---\n",
    "    if verbose:\n",
    "        print(\"=\" * 60)\n",
    "        print(\"CROSS-VALIDATION EVALUATION\")\n",
    "        print(\"=\" * 60)\n",
    "    \n",
    "    pipeline = Pipeline(steps=[\n",
    "        ('smote', SMOTE(random_state=random_state, sampling_strategy=smote_strategy)),\n",
    "        ('lgbm', LGBMClassifier(**best_params))\n",
    "    ])\n",
    "    \n",
    "    skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=random_state)\n",
    "    scorer = make_scorer(balanced_accuracy_score)\n",
    "    cv_scores = cross_val_score(pipeline, X_train, y_train, cv=skf, scoring=scorer, n_jobs=-1)\n",
    "    \n",
    "    if verbose:\n",
    "        print(f\"Balanced Accuracy per fold: {np.round(cv_scores, 4)}\")\n",
    "        print(f\"Mean Balanced Accuracy: {np.round(np.mean(cv_scores), 4)} ± {np.round(np.std(cv_scores), 4)}\")\n",
    "        print()\n",
    "    \n",
    "    # --- 4. Train final model with SMOTE applied on training set ---\n",
    "    if verbose:\n",
    "        print(\"=\" * 60)\n",
    "        print(\"FINAL MODEL TRAINING\")\n",
    "        print(\"=\" * 60)\n",
    "    \n",
    "    smote = SMOTE(random_state=random_state, sampling_strategy=smote_strategy)\n",
    "    X_train_bal, y_train_bal = smote.fit_resample(X_train, y_train)\n",
    "    \n",
    "    if verbose:\n",
    "        print(f\"After SMOTE - Train size: {len(X_train_bal)}\")\n",
    "        print(f\"Class distribution (balanced): {dict(pd.Series(y_train_bal).value_counts().sort_index())}\")\n",
    "        print()\n",
    "    \n",
    "    model = LGBMClassifier(**best_params)\n",
    "    \n",
    "    # Fit with or without early stopping\n",
    "    if use_early_stopping:\n",
    "        callbacks = [lgb.early_stopping(early_stopping_rounds, verbose=False)]\n",
    "        model.fit(\n",
    "            X_train_bal, y_train_bal,\n",
    "            eval_set=[(X_test, y_test)],\n",
    "            callbacks=callbacks\n",
    "        )\n",
    "        if verbose and hasattr(model, 'best_iteration_'):\n",
    "            print(f\"Best iteration: {model.best_iteration_} (out of {best_params['n_estimators']})\")\n",
    "    else:\n",
    "        model.fit(X_train_bal, y_train_bal)\n",
    "    \n",
    "    # --- 5. Feature importance ---\n",
    "    feature_importance = pd.DataFrame({\n",
    "        'feature': X_train.columns,\n",
    "        'importance': model.feature_importances_\n",
    "    }).sort_values('importance', ascending=False)\n",
    "    \n",
    "    # --- 6. Test set evaluation (before retraining) ---\n",
    "    y_pred = model.predict(X_test)\n",
    "    test_balanced_accuracy = balanced_accuracy_score(y_test, y_pred)\n",
    "    \n",
    "    if verbose:\n",
    "        print(f\"\\nTest Set Balanced Accuracy: {np.round(test_balanced_accuracy, 4)}\")\n",
    "        print(\"\\nClassification Report (Test Set):\")\n",
    "        print(classification_report(y_test, y_pred))\n",
    "        print(\"\\nTop 10 Most Important Features:\")\n",
    "        print(feature_importance.head(10).to_string(index=False))\n",
    "    \n",
    "    # Save predictions before retraining\n",
    "    test_predictions = y_pred.copy()\n",
    "    \n",
    "    # --- 7. Retrain on full dataset ---\n",
    "    if verbose:\n",
    "        print(\"\\n\" + \"=\" * 60)\n",
    "        print(\"RETRAINING ON FULL DATASET (TRAIN + TEST)\")\n",
    "        print(\"=\" * 60)\n",
    "    \n",
    "    # Combine train and test\n",
    "    X_full = pd.concat([X_train, X_test], axis=0)\n",
    "    y_full = pd.concat([y_train, y_test], axis=0)\n",
    "    \n",
    "    # Apply SMOTE to full dataset\n",
    "    smote_full = SMOTE(random_state=random_state, sampling_strategy=smote_strategy)\n",
    "    X_full_bal, y_full_bal = smote_full.fit_resample(X_full, y_full)\n",
    "    \n",
    "    if verbose:\n",
    "        print(f\"Full dataset size: {len(X_full_bal)}\")\n",
    "        print(f\"Class distribution: {dict(pd.Series(y_full_bal).value_counts().sort_index())}\")\n",
    "    \n",
    "    # Retrain model on everything\n",
    "    final_model = LGBMClassifier(**best_params)\n",
    "    final_model.fit(X_full_bal, y_full_bal)\n",
    "    \n",
    "    if verbose:\n",
    "        print(\"✓ Model retrained on full dataset\")\n",
    "        print(\"=\" * 60)\n",
    "    \n",
    "    # Return final_model with test predictions from before retraining\n",
    "    return (final_model, X_train, X_test, y_train, y_test, cv_scores, \n",
    "            test_balanced_accuracy, feature_importance, best_params, test_predictions)\n",
    "\n",
    "\n",
    "# Example usage:\n",
    "\"\"\"\n",
    "model, X_train, X_test, y_train, y_test, cv_scores, test_balanced_accuracy, feature_importance, hyperparameters, predictions = evaluate_with_smote_cv_lightgbm_optimal2(\n",
    "    X, y,\n",
    "    test_size=0.2,\n",
    "    n_splits=5,\n",
    "    random_state=42,\n",
    "    use_early_stopping=True,\n",
    "    early_stopping_rounds=50,\n",
    "    use_class_weights=False,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "# Now you can use variables directly\n",
    "print(f\"Model: {model}\")\n",
    "print(f\"CV Scores: {cv_scores}\")\n",
    "print(f\"Test Accuracy: {test_balanced_accuracy}\")\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4c4c6d4a-9fe5-4c92-886f-042b6519b750",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "TRAINING CONFIGURATION\n",
      "============================================================\n",
      "Train size: 1678, Test size: 420\n",
      "Class distribution (train): {1.0: 1308, 2.0: 233, 3.0: 137}\n",
      "Class distribution (test): {1.0: 327, 2.0: 58, 3.0: 35}\n",
      "\n",
      "============================================================\n",
      "CROSS-VALIDATION EVALUATION\n",
      "============================================================\n",
      "Balanced Accuracy per fold: [0.8872 0.9156 0.9035 0.9348 0.9395]\n",
      "Mean Balanced Accuracy: 0.9161 ± 0.0195\n",
      "\n",
      "============================================================\n",
      "FINAL MODEL TRAINING\n",
      "============================================================\n",
      "After SMOTE - Train size: 3924\n",
      "Class distribution (balanced): {1.0: 1308, 2.0: 1308, 3.0: 1308}\n",
      "\n",
      "Best iteration: 295 (out of 295)\n",
      "\n",
      "Test Set Balanced Accuracy: 0.9135\n",
      "\n",
      "Classification Report (Test Set):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         1.0       0.97      0.93      0.95       327\n",
      "         2.0       0.71      0.90      0.79        58\n",
      "         3.0       0.91      0.91      0.91        35\n",
      "\n",
      "    accuracy                           0.92       420\n",
      "   macro avg       0.87      0.91      0.89       420\n",
      "weighted avg       0.93      0.92      0.93       420\n",
      "\n",
      "\n",
      "Top 10 Most Important Features:\n",
      " feature  importance\n",
      "    ASTV        1307\n",
      "    ALTV         787\n",
      "  Median         599\n",
      "      UC         569\n",
      "      AC         559\n",
      "    Mode         506\n",
      "      DP         496\n",
      "Variance         477\n",
      "\n",
      "============================================================\n",
      "RETRAINING ON FULL DATASET (TRAIN + TEST)\n",
      "============================================================\n",
      "Full dataset size: 4905\n",
      "Class distribution: {1.0: 1635, 2.0: 1635, 3.0: 1635}\n",
      "✓ Model retrained on full dataset\n",
      "============================================================\n",
      "Model: LGBMClassifier(bagging_fraction=0.9098171071119672, bagging_freq=1,\n",
      "               extra_trees=False, feature_fraction=0.9596036089327271,\n",
      "               lambda_l1=0.799084665275944, lambda_l2=0.7644980529191521,\n",
      "               learning_rate=0.015148189601668145, max_depth=3,\n",
      "               min_child_samples=25, min_split_gain=0.44871424998167664,\n",
      "               n_estimators=295, n_jobs=-1, num_class=3, num_leaves=24,\n",
      "               objective='multiclass', random_state=42, verbose=-1)\n",
      "CV Scores: [0.88716647 0.91564956 0.90349336 0.93484409 0.93953618]\n",
      "Test Accuracy: 0.9135003489954454\n"
     ]
    }
   ],
   "source": [
    "\n",
    "model, X_train, X_test, y_train, y_test, cv_scores, test_balanced_accuracy, feature_importance, hyperparameters, predictions = evaluate_with_smote_cv_lightgbm_optimal2(\n",
    "    X, y,\n",
    "    test_size=0.2,\n",
    "    n_splits=5,\n",
    "    random_state=42,\n",
    "    use_early_stopping=True,\n",
    "    early_stopping_rounds=50,\n",
    "    use_class_weights=False,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "# Now you can use variables directly\n",
    "print(f\"Model: {model}\")\n",
    "print(f\"CV Scores: {cv_scores}\")\n",
    "print(f\"Test Accuracy: {test_balanced_accuracy}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a6adaabc-4c91-4806-b08c-7370223990f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== First 10 Samples: Actual vs Predicted NSP ===\n",
      "    Variance   DP   AC    UC   Mode  Median  ASTV  ALTV  Actual_NSP  \\\n",
      "0       73.0  0.0  0.0   0.0  120.0   121.0  73.0  43.0         2.0   \n",
      "1       12.0  0.0  4.0   4.0  141.0   140.0  17.0   0.0         1.0   \n",
      "2       13.0  0.0  2.0   5.0  141.0   138.0  16.0   0.0         1.0   \n",
      "3       13.0  0.0  2.0   6.0  137.0   137.0  16.0   0.0         1.0   \n",
      "4       11.0  0.0  4.0   5.0  137.0   138.0  16.0   0.0         1.0   \n",
      "5      170.0  2.0  1.0  10.0   76.0   107.0  26.0   0.0         3.0   \n",
      "6      215.0  2.0  1.0   9.0   71.0   106.0  29.0   0.0         3.0   \n",
      "7        3.0  0.0  0.0   0.0  122.0   123.0  83.0   6.0         3.0   \n",
      "8        3.0  0.0  0.0   1.0  122.0   123.0  84.0   5.0         3.0   \n",
      "9        1.0  0.0  0.0   3.0  122.0   123.0  86.0   6.0         3.0   \n",
      "10       9.0  0.0  0.0   1.0  150.0   151.0  64.0   9.0         2.0   \n",
      "11      10.0  0.0  0.0   1.0  150.0   151.0  64.0   8.0         2.0   \n",
      "12       7.0  0.0  4.0   6.0  135.0   137.0  28.0   0.0         1.0   \n",
      "13      10.0  0.0  6.0   4.0  141.0   141.0  28.0   0.0         1.0   \n",
      "14      76.0  1.0  7.0   5.0  143.0   135.0  21.0   0.0         1.0   \n",
      "15      43.0  1.0  4.0   3.0  134.0   133.0  19.0   0.0         1.0   \n",
      "16      70.0  0.0  4.0   3.0  143.0   138.0  24.0   0.0         1.0   \n",
      "17      45.0  1.0  1.0   2.0  134.0   132.0  18.0   0.0         2.0   \n",
      "18      36.0  1.0  2.0   4.0  133.0   129.0  23.0   0.0         1.0   \n",
      "19      27.0  1.0  6.0   6.0  133.0   133.0  29.0   0.0         1.0   \n",
      "20     138.0  3.0  0.0   4.0  129.0   120.0  30.0   0.0         3.0   \n",
      "21      34.0  1.0  3.0   2.0  129.0   132.0  26.0   0.0         1.0   \n",
      "22     148.0  2.0  0.0   2.0   75.0   102.0  34.0   0.0         3.0   \n",
      "23       1.0  0.0  0.0   0.0  126.0   125.0  80.0   0.0         3.0   \n",
      "24       0.0  0.0  0.0   2.0  128.0   129.0  86.0  79.0         3.0   \n",
      "25       0.0  0.0  0.0   0.0  124.0   125.0  86.0  72.0         3.0   \n",
      "26       0.0  0.0  0.0   0.0  126.0   127.0  86.0  14.0         3.0   \n",
      "27       0.0  0.0  0.0   0.0  124.0   125.0  87.0  71.0         3.0   \n",
      "28      73.0  1.0  0.0   1.0  133.0   129.0  29.0   0.0         2.0   \n",
      "29      89.0  0.0  0.0   0.0  133.0   117.0  26.0   0.0         1.0   \n",
      "\n",
      "    Predicted_NSP  Prob_Class_1  Prob_Class_2  Prob_Class_3  \n",
      "0             3.0      0.023031      0.473927      0.503041  \n",
      "1             1.0      0.991693      0.005234      0.003073  \n",
      "2             1.0      0.985591      0.010687      0.003722  \n",
      "3             1.0      0.985450      0.010828      0.003722  \n",
      "4             1.0      0.990893      0.005901      0.003206  \n",
      "5             3.0      0.011650      0.032750      0.955599  \n",
      "6             3.0      0.004367      0.013596      0.982037  \n",
      "7             3.0      0.019646      0.030832      0.949521  \n",
      "8             3.0      0.027404      0.034306      0.938290  \n",
      "9             3.0      0.075810      0.008274      0.915916  \n",
      "10            2.0      0.021579      0.966049      0.012372  \n",
      "11            2.0      0.021579      0.966049      0.012372  \n",
      "12            1.0      0.990893      0.005901      0.003206  \n",
      "13            1.0      0.991820      0.005136      0.003044  \n",
      "14            1.0      0.816932      0.155462      0.027606  \n",
      "15            1.0      0.923633      0.060041      0.016326  \n",
      "16            1.0      0.989310      0.006065      0.004625  \n",
      "17            2.0      0.406340      0.520822      0.072839  \n",
      "18            1.0      0.900679      0.075905      0.023417  \n",
      "19            1.0      0.946204      0.042207      0.011589  \n",
      "20            3.0      0.009885      0.015804      0.974310  \n",
      "21            1.0      0.881988      0.095979      0.022033  \n",
      "22            3.0      0.004225      0.007465      0.988310  \n",
      "23            3.0      0.020410      0.022345      0.957246  \n",
      "24            3.0      0.023038      0.018726      0.958236  \n",
      "25            3.0      0.006602      0.009408      0.983990  \n",
      "26            3.0      0.014027      0.026313      0.959661  \n",
      "27            3.0      0.006602      0.009408      0.983990  \n",
      "28            2.0      0.426968      0.450949      0.122084  \n",
      "29            1.0      0.865667      0.045457      0.088876  \n",
      "\n",
      "Correct Predictions: 29/10\n"
     ]
    }
   ],
   "source": [
    "# --- Select the first 10 samples ---\n",
    "X_sample = X.head(30)\n",
    "y_sample = y.head(30).reset_index(drop=True)\n",
    "\n",
    "# --- Make predictions ---\n",
    "preds = model.predict(X_sample)\n",
    "probs = model.predict_proba(X_sample)\n",
    "\n",
    "# --- Combine everything into one table ---\n",
    "pred_df = X_sample.copy()\n",
    "pred_df['Actual_NSP'] = y_sample\n",
    "pred_df['Predicted_NSP'] = preds\n",
    "pred_df['Prob_Class_1'] = probs[:, 0]\n",
    "pred_df['Prob_Class_2'] = probs[:, 1]\n",
    "pred_df['Prob_Class_3'] = probs[:, 2]\n",
    "\n",
    "# --- Display neatly ---\n",
    "print(\"\\n=== First 10 Samples: Actual vs Predicted NSP ===\")\n",
    "print(pred_df)\n",
    "\n",
    "# --- Optional: display concise summary of prediction correctness ---\n",
    "correct = (pred_df['Actual_NSP'] == pred_df['Predicted_NSP']).sum()\n",
    "print(f\"\\nCorrect Predictions: {correct}/10\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
